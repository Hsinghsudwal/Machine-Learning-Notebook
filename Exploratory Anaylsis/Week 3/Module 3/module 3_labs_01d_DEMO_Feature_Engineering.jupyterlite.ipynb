{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Pyolite",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Machine Learning Foundation\n\n## Section 1, Part d: Feature Engineering\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Feature Engineering with Linear Regression: Applied to the Ames Housing Data\n\nUsing the Ames Housing Data:\n\nDean De Cock\nTruman State University\nJournal of Statistics Education Volume 19, Number 3(2011), [www.amstat.org/publications/jse/v19n3/decock.pdf](http://www.amstat.org/publications/jse/v19n3/decock.pdf?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01)\n\nIn this notebook, we will build some linear regression models to predict housing prices from this data. In particular, we will set out to improve on a baseline set of features via **feature engineering**: deriving new features from our existing data. Feature engineering often makes the difference between a weak model and a strong one.\n\nWe will use visual exploration, domain understanding, and intuition to construct new features that will be useful later in the course as we turn to prediction.\n\n**Notebook Contents**\n\n> 1.  Simple EDA\n> 2.  One-hot Encoding variables\n> 3.  Log transformation for skewed variables\n> 4.  Pair plot for features\n> 5.  Basic feature engineering: adding polynomial and interaction terms\n> 6.  Feature engineering: categories and features derived from category aggregates\n\n## 1. Simple EDA\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import piplite\nawait piplite.install(['pandas', 'skillsnetwork', 'seaborn'])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "%pylab inline\n%config InlineBackend.figure_formats = ['retina']\n\nimport pandas as pd\nimport seaborn as sns\nimport skillsnetwork\nsns.set()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Load the Data, Examine and Explore\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "## Load in the Ames Housing Data\nURL = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/Ames_Housing_Data.tsv'\n\nawait skillsnetwork.download_dataset(URL)\ndf = pd.read_csv('Ames_Housing_Data.tsv', sep='\\t')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Examine the columns, look at missing data\ndf.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# This is recommended by the data set author to remove a few outliers\n\ndf = df.loc[df['Gr Liv Area'] <= 4000,:]\nprint(\"Number of rows in the data:\", df.shape[0])\nprint(\"Number of columns in the data:\", df.shape[1])\ndata = df.copy() # Keep a copy our original data ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# A quick look at the data:\ndf.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We're going to first do some basic data cleaning on this data:\n\n*   Converting categorical variables to dummies\n*   Making skew variables symmetric\n\n### One-hot encoding for dummy variables:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get a Pd.Series consisting of all the string categoricals\none_hot_encode_cols = df.dtypes[df.dtypes == np.object]  # filtering by string categoricals\none_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n\ndf[one_hot_encode_cols].head().T",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We're going to first do some basic data cleaning on this data:\n\n*   Converting categorical variables to dummies\n*   Making skew variables symmetric\n\n#### One-hot encoding the dummy variables:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Do the one hot encoding\ndf = pd.get_dummies(df, columns=one_hot_encode_cols, drop_first=True)\ndf.describe().T",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Log transforming skew variables\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create a list of float colums to check for skewing\nmask = data.dtypes == np.float\nfloat_cols = data.columns[mask]\n\nskew_limit = 0.75 # define a limit above which we will log transform\nskew_vals = data[float_cols].skew()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Showing the skewed columns\nskew_cols = (skew_vals\n             .sort_values(ascending=False)\n             .to_frame()\n             .rename(columns={0:'Skew'})\n             .query('abs(Skew) > {}'.format(skew_limit)))\n\nskew_cols",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Let's look at what happens to one of these features, when we apply np.log1p visually.\n\n# Choose a field\nfield = \"BsmtFin SF 1\"\n\n# Create two \"subplots\" and a \"figure\" using matplotlib\nfig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Create a histogram on the \"ax_before\" subplot\ndf[field].hist(ax=ax_before)\n\n# Apply a log transformation (numpy syntax) to this column\ndf[field].apply(np.log1p).hist(ax=ax_after)\n\n# Formatting of titles etc. for each subplot\nax_before.set(title='before np.log1p', ylabel='frequency', xlabel='value')\nax_after.set(title='after np.log1p', ylabel='frequency', xlabel='value')\nfig.suptitle('Field \"{}\"'.format(field));",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Perform the skew transformation:\n\nfor col in skew_cols.index.values:\n    if col == \"SalePrice\":\n        continue\n    df[col] = df[col].apply(np.log1p)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# We now have a larger set of potentially-useful features\ndf.shape",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# There are a *lot* of variables. Let's go back to our saved original data and look at how many values are missing for each variable. \ndf = data\ndata.isnull().sum().sort_values()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Let's pick out just a few numeric columns to illustrate basic feature transformations.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n                      'Garage Cars','SalePrice']]",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Now we can look at summary statistics of the subset data\nsmaller_df.describe().T",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "smaller_df.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# There appears to be one NA in Garage Cars - we will take a simple approach and fill it with 0\nsmaller_df = smaller_df.fillna(0)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "smaller_df.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Pair plot of features\n\nNow that we have a nice, filtered dataset, let's generate visuals to better understand the target and feature-target relationships: pairplot is great for this!\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n**Data Exploration Discussion**: \n\n1. What do these plots tell us about the distribution of the target?   \n\n2. What do these plots tell us about the relationship between the features and the target? Do you think that linear regression is well-suited to this problem? Do any feature transformations come to mind?\n\n3. What do these plots tell us about the relationship between various pairs of features? Do you think there may be any problems here? \n\n---\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "#### Suppose our target variable is the SalePrice. We can set up separate variables for features and target.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "#Separate our features from our target\n\nX = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n                      'Garage Cars']]\n\ny = smaller_df['SalePrice']",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X.info()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "Now that we have feature/target data X, y ready to go, we're nearly ready to fit and evaluate a baseline model using our current feature set. We'll need to create a **train/validation split** before we fit and score the model.\n\nSince we'll be repeatedly splitting X, y into the same train/val partitions and fitting/scoring new models as we update our feature set, we'll define a reusable function that completes all these steps, making our code/process more efficient going forward.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Great, let's go ahead and run this function on our baseline feature set and take some time to analyze the results.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Basic feature engineering: adding polynomial and interaction terms\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "One of the first things that we looked for in the pairplot was evidence about the relationship between each feature and the target. In certain features like *'Overall Qual'* and *'Gr Liv Qual'*, we notice an upward-curved relationship rather than a simple linear correspondence. This suggests that we should add quadratic **polynomial terms or transformations** for those features, allowing us to express that non-linear relationship while still using linear regression as our model.\n\nLuckily, pandas makes it quite easy to quickly add those square terms as additional features to our original feature set. We'll do so and evaluate our model again below.\n\nAs we add to our baseline set of features, we'll create a copy of the latest benchmark so that we can continue to store our older feature sets.\n\n### Polynomial Features\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X2 = X.copy()\n\nX2['OQ2'] = X2['Overall Qual'] ** 2\nX2['GLA2'] = X2['Gr Liv Area'] ** 2\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "As is, each feature is treated as an independent quantity. However, there may be **interaction effects**, in which the impact of one feature may dependent on the current value of a different feature.\n\nFor example, there may be a higher premium for increasing *'Overall Qual'* for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies *'Overall Qual'* by *'Year Built'* can help us capture it.\n\nAnother style of interaction term involves feature proprtions: for example, to get at something like quality per square foot we could divide *'Overall Qual'* by *'Lot Area'*.\n\nLet's try adding both of these interaction terms and see how they impact the model results.\n\n### Feature interactions\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X3 = X2.copy()\n\n# multiplicative interaction\nX3['OQ_x_YB'] = X3['Overall Qual'] * X3['Year Built']\n\n# division interaction\nX3['OQ_/_LA'] = X3['Overall Qual'] / X3['Lot Area']\n\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "***\n\n**Interaction Feature Exercise**: What other interactions do you think might be helpful? Why?\n\n***\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Categories and features derived from category aggregates\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Incorporating **categorical features** into linear regression models is fairly straightforward: we can create a new feature column for each category value, and fill these columns with 1s and 0s to indicate which category is present for each row. This method is called **dummy variables** or **one-hot-encoding**.\n\nWe'll first explore this using the *'House Style'* feature from the original dataframe. Before going straight to dummy variables, it's a good idea to check category counts to make sure all categories have reasonable representation.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data['House Style'].value_counts()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "This looks ok, and here's a quick look at how dummy features actually appear:\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "pd.get_dummies(df['House Style'], drop_first=True).head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "We can call `pd.get_dummies()` on our entire dataset to quickly get data with all the original features and dummy variable representation of any categorical features. Let's look at some variable values.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "nbh_counts = df.Neighborhood.value_counts()\nnbh_counts",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "For this category, let's map the few least-represented neighborhoods to an \"other\" category before adding the feature to our feature set and running a new benchmark.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "other_nbhs = list(nbh_counts[nbh_counts <= 8].index)\n\nother_nbhs",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "X4 = X3.copy()\n\nX4['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "#### Getting to fancier features\n\nLet's close out our introduction to feature engineering by considering a more complex type of feature that may work very nicely for certain problems. It doesn't seem to add a great deal over what we have so far, but it's a style of engineering to keep in mind for the future.\n\nWe'll create features that capture where a feature value lies relative to the members of a category it belongs to. In particular, we'll calculate deviance of a row's feature value from the mean value of the category that row belongs to. This helps to capture information about a feature relative to the category's distribution, e.g. how nice a house is relative to other houses in its neighborhood or of its style.\n\nBelow we define reusable code for generating features of this form, feel free to repurpose it for future feature engineering work!\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def add_deviation_feature(X, feature, category):\n    \n    # temp groupby object\n    category_gb = X.groupby(category)[feature]\n    \n    # create category means and standard deviations for each observation\n    category_mean = category_gb.transform(lambda x: x.mean())\n    category_std = category_gb.transform(lambda x: x.std())\n    \n    # compute stds from category mean for each feature value,\n    # add to X as new feature\n    deviation_feature = (X[feature] - category_mean) / category_std \n    X[feature + '_Dev_' + category] = deviation_feature  ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "And now let's use our feature generation code to add 2 new deviation features, and run a final benchmark.\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X5 = X4.copy()\nX5['House Style'] = df['House Style']\nadd_deviation_feature(X5, 'Year Built', 'House Style')\nadd_deviation_feature(X5, 'Overall Qual', 'Neighborhood')\n",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Polynomial Features in Scikit-Learn\n\n`sklearn` allows you to build many higher-order terms at once with `PolynomialFeatures`\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import PolynomialFeatures",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Instantiate and provide desired degree; \n#   Note: degree=2 also includes intercept, degree 1 terms, and cross-terms\n\npf = PolynomialFeatures(degree=2)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "features = ['Lot Area', 'Overall Qual']\npf.fit(df[features])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "pf.get_feature_names()  #Must add input_features = features for appropriate names",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "feat_array = pf.transform(df[features])\npd.DataFrame(feat_array, columns = pf.get_feature_names(input_features=features))",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Recap\n\nWhile we haven't yet turned to prediction, these feature engineering exercises set the stage. Generally, feature engineering often follows a sort of [*Pareto principle*](https://en.wikipedia.org/wiki/Pareto_principle?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01), where a large bulk of the predictive gains can be reached through adding a set of intuitive, strong features like polynomial transforms and interactions. Directly incorporating additional information like categorical variables can also be very helpful. Beyond this point, additional feature engineering can provide significant, but potentially diminishing returns. Whether it's worth it depends on the use case for the model.\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "***\n\n### Machine Learning Foundation (C) 2020 IBM Corporation\n",
      "metadata": {}
    }
  ]
}